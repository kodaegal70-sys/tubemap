import requests
import json
import time
import os
import urllib.request
import urllib.parse
import re
import concurrent.futures

# API Keys
KAKAO_API_KEY = "c6088c2c7ec5f0e1ed1122ba613db0fb"
NAVER_CLIENT_ID = "Ug7csvlUDeTe1I72ehKQ"
NAVER_CLIENT_SECRET = "pqPjVw9kig"

# 18ëŒ€ ì§€ì¹¨: ì •í™•í•œ ë¯¸ë””ì–´ ëª…ì¹­ ë§¤í•‘
OFFICIAL_MEDIA_NAMES = {
    "ì„±ì‹œê²½": "ì„±ì‹œê²½ì˜ ë¨¹ì„í…ë°",
    "í’ì": "ë˜ê°„ì§‘",
    "ë°±ì¢…ì›": "ë‹˜ì•„ ê·¸ ì‹œì¥ì„ ê°€ì˜¤",
    "ì¯”ì–‘": "tzuyangì¯”ì–‘",
    "ì•¼ì‹ì´": "ì•¼ì‹ì´",
    "ìƒí•´ê¸°": "ìƒí•´ê¸°",
    "ë¨¹ì„í…ë°": "ì„±ì‹œê²½ì˜ ë¨¹ì„í…ë°",
    "ë°±ë°˜ê¸°í–‰": "ì‹ê° í—ˆì˜ë§Œì˜ ë°±ë°˜ê¸°í–‰",
    "ìƒí™œì˜ë‹¬ì¸": "ìƒí™œì˜ ë‹¬ì¸",
    "ë§›ìˆëŠ”ë…€ì„ë“¤": "ë§›ìˆëŠ” ë…€ì„ë“¤",
    "í† ìš”ì¼ì€ë°¥ì´ì¢‹ì•„": "í† ìš”ì¼ì€ ë°¥ì´ ì¢‹ì•„",
    "ì „ì°¸ì‹œ": "ì „ì§€ì  ì°¸ê²¬ ì‹œì ",
    "ë†€í† ": "ë†€ë¼ìš´ í† ìš”ì¼"
}

# 15ëŒ€ / 18ëŒ€ ì§€ì¹¨: êµ¬ë…ì 50ë§Œ ì´ìƒ ë©”ê°€ ì±„ë„ (Whitelist)
MEGA_CHANNELS = {
    "ì„±ì‹œê²½": ["ì„±ì‹œê²½", "ë¨¹ì„í…ë°"],
    "í’ì": ["í’ì", "ë˜ê°„ì§‘"],
    "ë°±ì¢…ì›": ["ë°±ì¢…ì›", "ë‹˜ì•„"],
    "ì¯”ì–‘": ["ì¯”ì–‘"],
    "ì•¼ì‹ì´": ["ì•¼ì‹ì´"],
    "ìƒí•´ê¸°": ["ìƒí•´ê¸°"],
    "ë°±ë°˜ê¸°í–‰": ["í—ˆì˜ë§Œ", "ë°±ë°˜ê¸°í–‰"],
    "ë§›ìˆëŠ”ë…€ì„ë“¤": ["ë§›ìˆëŠ”ë…€ì„ë“¤"],
    "ìƒí™œì˜ë‹¬ì¸": ["ìƒí™œì˜ë‹¬ì¸"]
}

VALID_CATEGORIES = ["ìŒì‹ì ", "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ë·”í˜", "ìˆ ì§‘", "ì¹´í˜"]
FRANCHISE_KEYWORDS = ["ìŠ¤íƒ€ë²…ìŠ¤", "ë§¥ë„ë‚ ë“œ", "ë¦¬ì•„", "í‚¹", "ì¨ë¸Œì›¨ì´", "íˆ¬ì¸", "íŒŒìŠ¤ì¿ ì°Œ", "ì´ë””ì•¼", "ë©”ê°€ì»¤í”¼", "ì»´í¬ì¦ˆ", "ë¹½ë‹¤ë°©", "ë°°ìŠ¤í‚¨", "ë˜í‚¨", "íŒŒë¦¬ë°”ê²Œëœ¨", "ëšœë ˆì¥¬ë¥´", "ì„œê°€ì•¤ì¿¡", "ì•„ì›ƒë°±", "ë¹•ìŠ¤", "ë³¸ì£½", "í•œì†¥", "ë´‰êµ¬ìŠ¤", "BBQ", "BHC", "êµì´Œ"]

BAD_DOMAINS = ["shopping", "smartstore", "coupang", "11st", "gmarket", "auction", "wemakeprice", "tmon", "shop.phinf", "map.naver", "tmap.co.kr"]
BAD_KEYWORDS_IN_TITLE = ["ë°€í‚¤íŠ¸", "í¬ì¥", "íƒë°°", "ê³µêµ¬", "íŒë§¤", "ì¶œì‹œ", "ìŠ¤í† ì–´", "ì§€ë„", "ì•½ë„", "ìœ„ì¹˜", "ê°€ëŠ”ê¸¸", "ë¡œë“œë·°", "ìº¡ì²˜"]

def sanitize_category(raw_cat):
    if not raw_cat: return "í•œì‹" 
    if raw_cat in ["í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ì¹´í˜", "ìˆ ì§‘"]: return raw_cat
    rc = raw_cat.replace(" ", "")
    if "ì¤‘êµ­" in rc or "ë§ˆë¼" in rc: return "ì¤‘ì‹"
    if "ì¼ì‹" in rc or "ì´ˆë°¥" in rc or "ìŠ¤ì‹œ" in rc or "ë¼ë©˜" in rc or "ëˆê°€ìŠ¤" in rc: return "ì¼ì‹"
    if "í”¼ì" in rc or "íŒŒìŠ¤íƒ€" in rc or "ë²„ê±°" in rc or "ì¹˜í‚¨" in rc or "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ" in rc or "ìŠ¤í…Œì´í¬" in rc or "ë ˆìŠ¤í† ë‘" in rc: return "ì–‘ì‹"
    if "ë–¡ë³¶ì´" in rc or "ê¹€ë°¥" in rc or "ìˆœëŒ€" in rc or "íŠ€ê¹€" in rc: return "ë¶„ì‹"
    if "ì»¤í”¼" in rc or "ë””ì €íŠ¸" in rc or "ë² ì´ì»¤ë¦¬" in rc or "ì œê³¼" in rc: return "ì¹´í˜"
    return "í•œì‹"

def get_kakao_place_info(query):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": query, "size": 3} 
    try:
        res = requests.get(url, headers=headers, params=params)
        data = res.json()
        if not data.get('documents'): return None
        matches = data['documents']
        best_match = matches[0]
        cat_name = best_match.get('category_name', '')
        if any(exc in cat_name for exc in ["ìŠˆí¼ë§ˆì¼“", "ëŒ€í˜•ë§ˆíŠ¸", "í¸ì˜ì "]): return None
        if any(fk in best_match['place_name'] for fk in FRANCHISE_KEYWORDS): return None
        cats = cat_name.split('>')
        detail_cat = cats[-1].strip() if len(cats) > 0 else "ìŒì‹ì "
        standard_cat = sanitize_category(detail_cat)
        return {
            "name": best_match['place_name'],
            "lat": float(best_match['y']), "lng": float(best_match['x']),
            "address": best_match['address_name'], "phone": best_match.get('phone', ''),
            "category": standard_cat, 
            "category_group": cat_name,
            "road_address": best_match.get('road_address_name', '')
        }
    except: return None

def search_blog_first(keyword):
    encText = urllib.parse.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display=20&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    candidates = []
    try:
        res = urllib.request.urlopen(req)
        data = json.loads(res.read().decode('utf-8'))
        for item in data['items']:
            title = item['title'].replace('<b>', '').replace('</b>', '')
            found_names = re.findall(r'\[(.*?)\]|\"(.*?)\"|\'(.*?)\'|\<(.*?)\>', title)
            for groups in found_names:
                for name in groups:
                    if name and 2 <= len(name) <= 15:
                         if any(c in name for c in MEGA_CHANNELS.keys()): continue
                         candidates.append(name.strip())
    except: pass
    return list(set(candidates))

def get_representative_menu(place_name, kaka_cat, city):
    return kaka_cat

def search_image_api(query, sort_type='sim'):
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/image?query={encText}&display=5&sort={sort_type}&filter=medium" 
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(req)
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            for item in data['items']:
                link = item['link']
                title = item['title']
                # 21ëŒ€ ì§€ì¹¨: Anti-Ad Check
                if any(bd in link for bd in BAD_DOMAINS): continue
                if any(bk in title for bk in BAD_KEYWORDS_IN_TITLE): continue
                return link
    except: pass
    return None

def get_best_image_18_step(name, menu, city):
    # 21ëŒ€ ì§€ì¹¨: "ë°©ë¬¸í›„ê¸°" í‚¤ì›Œë“œ ì¶”ê°€ + Anti-Ad
    query1 = f"{name} {city} {menu} ë°©ë¬¸í›„ê¸° -ë°€í‚¤íŠ¸ -íƒë°° -í¬ì¥ -ìŠ¤í† ì–´" 
    img = search_image_api(query1, 'sim')
    if img: return img
    
    query2 = f"{name} {menu} ë°©ë¬¸í›„ê¸° -ë°€í‚¤íŠ¸"
    img = search_image_api(query2, 'sim')
    if img: return img
    
    return None

def process_candidate(cand_name, media_hint, seen_keys, existing_count):
    place_info = get_kakao_place_info(cand_name)
    if not place_info: return None
    
    if place_info['category'] not in ["í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ì¹´í˜", "ìˆ ì§‘"]:
        return None 
    
    key = f"{place_info['name']}_{place_info['address']}"
    if key in seen_keys: return None

    media_final = ""
    for k, v in OFFICIAL_MEDIA_NAMES.items():
        if k in media_hint or media_hint in k:
            media_final = v; break
            
    if not media_final: 
        for ch, kws in MEGA_CHANNELS.items():
            if ch in media_hint: 
                media_final = OFFICIAL_MEDIA_NAMES.get(ch, ch)
                break
    if not media_final: return None 

    city = place_info['address'].split()[1] if len(place_info['address'].split())>1 else ""
    menu = place_info['category'] 
    
    image_url = get_best_image_18_step(place_info['name'], menu, city)
    
    if not image_url:
        print(f"  âŒ {place_info['name']}: ì´ë¯¸ì§€ ê²€ì¦ ì‹¤íŒ¨ (Zero Tolerance)")
        return None 
    
    place_info['media'] = media_final
    place_info['description'] = f"{menu} ì „ë¬¸ì ."
    
    place_info['image_url'] = image_url
    place_info['naver_url'] = f"https://map.naver.com/p/search/{urllib.parse.quote(place_info['name'])}"
    
    addr_parts = place_info['address'].split(' ')
    place_info['addressProvince'] = addr_parts[0] if len(addr_parts) > 0 else ""
    place_info['addressCity'] = addr_parts[1] if len(addr_parts) > 1 else ""
    place_info['addressDistrict'] = addr_parts[2] if len(addr_parts) > 2 else ""
    
    return place_info

def collect_main():
    print("ğŸš€ [Phase 42] 21ëŒ€ ì§€ì¹¨ ìˆ˜ì§‘ ì—”ì§„ (ì´ë¯¸ì§€ ë¬´ê²°ì„± ê°•í™”)")
    
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    src_path = os.path.join(root_dir, 'src', 'data', 'places.json')
    
    try:
        with open(src_path, 'r', encoding='utf-8') as f:
            existing_places = json.load(f)
    except:
        existing_places = []
        
    seen_keys = set()
    for p in existing_places:
        seen_keys.add(f"{p['name']}_{p['address']}")
    
    new_places = []
    
    search_keywords = []
    for ytb, kws in MEGA_CHANNELS.items():
        main_kw = kws[1] if len(kws) > 1 else kws[0]
        search_keywords.append(f"{main_kw} ë§›ì§‘ ì¶”ì²œ")

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        for seed in search_keywords:
            if len(new_places) >= 5: break 
            
            candidates = search_blog_first(seed)
            if not candidates: continue
            media_hint = seed.split()[0]
            
            futures = []
            for cand in candidates:
                futures.append(executor.submit(process_candidate, cand, media_hint, seen_keys, len(new_places)))
            
            for f in concurrent.futures.as_completed(futures):
                res = f.result()
                if res:
                    key = f"{res['name']}_{res['address']}"
                    if key not in seen_keys:
                        res['id'] = len(existing_places) + len(new_places) + 1
                        new_places.append(res)
                        seen_keys.add(key)
                        print(f"  âœ¨ [New] {res['name']} ({res['category']}) - IMG OK")

    if new_places:
        with open(src_path, 'w', encoding='utf-8') as f:
            json.dump(existing_places + new_places, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ì‹ ê·œ {len(new_places)}ê°œ ì¶”ê°€ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        print("\nâœ… ì‹ ê·œ ìˆ˜ì§‘ ì—†ìŒ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€).")
        
if __name__ == "__main__":
    collect_main()
