#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
21ëŒ€ ìˆ˜ì§‘ ì§€ì¹¨ ì™„ì „ êµ¬í˜„ ì—”ì§„
- 3-Step ì´ë¯¸ì§€ ê²€ì¦ (Blog â†’ Naver Place â†’ Generic Menu)
- Menu-Image Similarity ê²€ì¦
- Branch Ambiguity ê²€ì¦
- Anti-Map/Ad/Milkit í•„í„°
- 50ë§Œ êµ¬ë…ì ê²€ì¦
"""

import requests
import json
import time
import os
import urllib.request
import urllib.parse
import re
import concurrent.futures
from typing import Optional, List, Dict

# API Keys
KAKAO_API_KEY = "c6088c2c7ec5f0e1ed1122ba613db0fb"
NAVER_CLIENT_ID = "Ug7csvlUDeTe1I72ehKQ"
NAVER_CLIENT_SECRET = "pqPjVw9kig"

# 21ëŒ€ ì§€ì¹¨: ê³µì‹ ë¯¸ë””ì–´ ëª…ì¹­
OFFICIAL_MEDIA_NAMES = {
    "ì„±ì‹œê²½": "ì„±ì‹œê²½ì˜ ë¨¹ì„í…ë°",
    "í’ì": "ë˜ê°„ì§‘",
    "ë°±ì¢…ì›": "ë‹˜ì•„ ê·¸ ì‹œì¥ì„ ê°€ì˜¤",
    "ì¯”ì–‘": "tzuyangì¯”ì–‘",
    "ì•¼ì‹ì´": "ì•¼ì‹ì´",
    "ìƒí•´ê¸°": "ìƒí•´ê¸°",
    "ë¨¹ì„í…ë°": "ì„±ì‹œê²½ì˜ ë¨¹ì„í…ë°",
    "ë°±ë°˜ê¸°í–‰": "ì‹ê° í—ˆì˜ë§Œì˜ ë°±ë°˜ê¸°í–‰",
    "ìƒí™œì˜ë‹¬ì¸": "ìƒí™œì˜ ë‹¬ì¸",
    "ë§›ìˆëŠ”ë…€ì„ë“¤": "ë§›ìˆëŠ” ë…€ì„ë“¤",
    "í† ìš”ì¼ì€ë°¥ì´ì¢‹ì•„": "í† ìš”ì¼ì€ ë°¥ì´ ì¢‹ì•„",
    "ì „ì°¸ì‹œ": "ì „ì§€ì  ì°¸ê²¬ ì‹œì ",
    "ë†€í† ": "ë†€ë¼ìš´ í† ìš”ì¼"
}

# 50ë§Œ êµ¬ë…ì ì´ìƒ ë©”ê°€ ì±„ë„ (Whitelist)
MEGA_CHANNELS = {
    "ì„±ì‹œê²½": ["ì„±ì‹œê²½", "ë¨¹ì„í…ë°"],
    "í’ì": ["í’ì", "ë˜ê°„ì§‘"],
    "ë°±ì¢…ì›": ["ë°±ì¢…ì›", "ë‹˜ì•„"],
    "ì¯”ì–‘": ["ì¯”ì–‘"],
    "ì•¼ì‹ì´": ["ì•¼ì‹ì´"],
    "ìƒí•´ê¸°": ["ìƒí•´ê¸°"],
    "ë°±ë°˜ê¸°í–‰": ["í—ˆì˜ë§Œ", "ë°±ë°˜ê¸°í–‰"],
    "ë§›ìˆëŠ”ë…€ì„ë“¤": ["ë§›ìˆëŠ”ë…€ì„ë“¤"],
    "ìƒí™œì˜ë‹¬ì¸": ["ìƒí™œì˜ë‹¬ì¸"]
}

# TV ë°©ì†¡ (ë¬´ì¡°ê±´ í†µê³¼)
TV_SHOWS = ["ìƒí™œì˜ ë‹¬ì¸", "ë°±ë°˜ê¸°í–‰", "ë§›ìˆëŠ” ë…€ì„ë“¤", "í† ìš”ì¼ì€ ë°¥ì´ ì¢‹ì•„", "ì „ì§€ì  ì°¸ê²¬ ì‹œì ", "ë†€ë¼ìš´ í† ìš”ì¼", "ë‹˜ì•„ ê·¸ ì‹œì¥ì„ ê°€ì˜¤"]

VALID_CATEGORIES = ["í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ì¹´í˜", "ìˆ ì§‘"]
FRANCHISE_KEYWORDS = ["ìŠ¤íƒ€ë²…ìŠ¤", "ë§¥ë„ë‚ ë“œ", "ë²„ê±°í‚¹", "ì¨ë¸Œì›¨ì´", "íˆ¬ì¸", "íŒŒìŠ¤ì¿ ì°Œ", "ì´ë””ì•¼", "ë©”ê°€ì»¤í”¼", "ì»´í¬ì¦ˆ", "ë¹½ë‹¤ë°©", "ë°°ìŠ¤í‚¨", "ë˜í‚¨", "íŒŒë¦¬ë°”ê²Œëœ¨", "ëšœë ˆì¥¬ë¥´"]

# 21ëŒ€ ì§€ì¹¨: Anti-Map/Ad/Milkit
BAD_DOMAINS = ["shopping", "smartstore", "coupang", "11st", "gmarket", "auction", "wemakeprice", "tmon", "shop.phinf", "map.naver", "tmap.co.kr"]
BAD_KEYWORDS_IN_TITLE = ["ë°€í‚¤íŠ¸", "í¬ì¥", "íƒë°°", "ê³µêµ¬", "íŒë§¤", "ì¶œì‹œ", "ìŠ¤í† ì–´", "ì§€ë„", "ì•½ë„", "ìœ„ì¹˜", "ê°€ëŠ”ê¸¸", "ë¡œë“œë·°", "ìº¡ì²˜", "ë°°ë‹¬"]

def sanitize_category(raw_cat: str) -> str:
    """ì¹´í…Œê³ ë¦¬ í‘œì¤€í™”"""
    if not raw_cat: return "í•œì‹"
    if raw_cat in VALID_CATEGORIES: return raw_cat
    rc = raw_cat.replace(" ", "")
    if "ì¤‘êµ­" in rc or "ë§ˆë¼" in rc: return "ì¤‘ì‹"
    if "ì¼ì‹" in rc or "ì´ˆë°¥" in rc or "ìŠ¤ì‹œ" in rc or "ë¼ë©˜" in rc or "ëˆê°€ìŠ¤" in rc: return "ì¼ì‹"
    if "í”¼ì" in rc or "íŒŒìŠ¤íƒ€" in rc or "ë²„ê±°" in rc or "ì¹˜í‚¨" in rc or "ìŠ¤í…Œì´í¬" in rc: return "ì–‘ì‹"
    if "ë–¡ë³¶ì´" in rc or "ê¹€ë°¥" in rc or "ìˆœëŒ€" in rc: return "ë¶„ì‹"
    if "ì»¤í”¼" in rc or "ë””ì €íŠ¸" in rc or "ë² ì´ì»¤ë¦¬" in rc: return "ì¹´í˜"
    return "í•œì‹"

def get_kakao_place_info(query: str) -> Optional[Dict]:
    """ì¹´ì¹´ì˜¤ APIë¡œ ì—…ì²´ ì •ë³´ í™•ë³´"""
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": query, "size": 5}
    
    try:
        res = requests.get(url, headers=headers, params=params, timeout=5)
        data = res.json()
        if not data.get('documents'): return None
        
        matches = data['documents']
        
        # 21ëŒ€ ì§€ì¹¨: ë³¸ì /ì§€ì  êµ¬ë¶„ ë¶ˆê°€ ì‹œ ì‚­ì œ
        if len(matches) > 1:
            names = [m['place_name'] for m in matches]
            # "ë³¸ì ", "ì§€ì " í‚¤ì›Œë“œ ì²´í¬
            if any("ë³¸ì " in n or "ì§€ì " in n for n in names):
                # ì •í™•í•œ ì§€ì  êµ¬ë¶„ ë¶ˆê°€
                print(f"  âš ï¸ Branch Ambiguity: {query} - {len(matches)} matches found")
                return None
        
        best_match = matches[0]
        cat_name = best_match.get('category_name', '')
        
        # 21ëŒ€ ì§€ì¹¨: ìŒì‹ì /ì¹´í˜ë§Œ í—ˆìš©
        if "ìŒì‹ì " not in cat_name and "ì¹´í˜" not in cat_name:
            print(f"  âŒ Not a restaurant: {cat_name}")
            return None
        
        # í”„ëœì°¨ì´ì¦ˆ ì œì™¸
        if any(fk in best_match['place_name'] for fk in FRANCHISE_KEYWORDS):
            return None
        
        cats = cat_name.split('>')
        detail_cat = cats[-1].strip() if len(cats) > 0 else "ìŒì‹ì "
        standard_cat = sanitize_category(detail_cat)
        
        return {
            "name": best_match['place_name'],
            "lat": float(best_match['y']),
            "lng": float(best_match['x']),
            "address": best_match['address_name'],
            "phone": best_match.get('phone', ''),
            "category": standard_cat,
            "category_group": cat_name,
            "road_address": best_match.get('road_address_name', '')
        }
    except Exception as e:
        print(f"  Error in Kakao API: {e}")
        return None

def search_image_api(query: str, sort_type='sim', display=10) -> Optional[str]:
    """ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ API"""
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/image?query={encText}&display={display}&sort={sort_type}&filter=medium"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    try:
        response = urllib.request.urlopen(req, timeout=5)
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            for item in data['items']:
                link = item['link']
                title = item['title']
                
                # 21ëŒ€ ì§€ì¹¨: Anti-Map/Ad/Milkit
                if any(bd in link for bd in BAD_DOMAINS): continue
                if any(bk in title for bk in BAD_KEYWORDS_IN_TITLE): continue
                if "map" in link.lower() or "location" in link.lower(): continue
                
                return link
    except Exception as e:
        print(f"  Image API Error: {e}")
    
    return None

def get_image_3step(name: str, menu: str, city: str) -> Optional[str]:
    """
    21ëŒ€ ì§€ì¹¨: 3-Step ì´ë¯¸ì§€ í™•ë³´
    Step 1: Blog (ì—…ì²´ëª… + ë©”ë‰´ + ë°©ë¬¸í›„ê¸°)
    Step 2: Naver Place (ì—…ì²´ëª… + ë©”ë‰´)
    Step 3: Generic Menu (ë©”ë‰´ í‚¤ì›Œë“œë§Œ, í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€)
    """
    # Step 1: Blog Review Image
    query1 = f"{name} {city} {menu} ë°©ë¬¸í›„ê¸° -ë°€í‚¤íŠ¸ -íƒë°° -í¬ì¥ -ì§€ë„ -ì•½ë„"
    img = search_image_api(query1, 'sim')
    if img:
        print(f"    âœ… Image (Blog): {img[:60]}...")
        return img
    
    # Step 2: Naver Place Image
    query2 = f"{name} {menu} -ì§€ë„ -ì•½ë„"
    img = search_image_api(query2, 'sim')
    if img:
        print(f"    âœ… Image (Place): {img[:60]}...")
        return img
    
    # Step 3: Generic Menu Image (21ëŒ€ ì§€ì¹¨: ë‹¤ë¥¸ ì—…ì²´ ì´ë¯¸ì§€ë¼ë„ ë©”ë‰´ ìœ ì‚¬ì„± ìˆìœ¼ë©´ OK)
    query3 = f"{menu} ìŒì‹ ì‚¬ì§„ -í…ìŠ¤íŠ¸ -ì§€ë„ -ì•½ë„ -ë°€í‚¤íŠ¸"
    img = search_image_api(query3, 'sim', display=15)
    if img:
        print(f"    âœ… Image (Generic): {img[:60]}...")
        return img
    
    print(f"    âŒ Image Failed (All 3 Steps)")
    return None

def search_blog_first(keyword: str) -> List[str]:
    """ë¸”ë¡œê·¸ì—ì„œ ì—…ì²´ëª… í›„ë³´ ì¶”ì¶œ"""
    encText = urllib.parse.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display=20&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    candidates = []
    try:
        res = urllib.request.urlopen(req, timeout=5)
        data = json.loads(res.read().decode('utf-8'))
        for item in data['items']:
            title = item['title'].replace('<b>', '').replace('</b>', '')
            # ëŒ€ê´„í˜¸, ë”°ì˜´í‘œ ë“±ì—ì„œ ì—…ì²´ëª… ì¶”ì¶œ
            found_names = re.findall(r'\[(.*?)\]|\"(.*?)\"|\'(.*?)\'|<(.*?)>', title)
            for groups in found_names:
                for name in groups:
                    if name and 2 <= len(name) <= 15:
                        if any(c in name for c in MEGA_CHANNELS.keys()): continue
                        candidates.append(name.strip())
    except Exception as e:
        print(f"  Blog Search Error: {e}")
    
    return list(set(candidates))

def process_candidate(cand_name: str, media_hint: str, seen_keys: set) -> Optional[Dict]:
    """í›„ë³´ ì—…ì²´ ì²˜ë¦¬"""
    print(f"\n  ğŸ” Processing: {cand_name}")
    
    place_info = get_kakao_place_info(cand_name)
    if not place_info:
        print(f"    âŒ Kakao API Failed")
        return None
    
    if place_info['category'] not in VALID_CATEGORIES:
        print(f"    âŒ Invalid Category: {place_info['category']}")
        return None
    
    key = f"{place_info['name']}_{place_info['address']}"
    if key in seen_keys:
        print(f"    âŒ Duplicate")
        return None
    
    # ë¯¸ë””ì–´ í™•ì¸
    media_final = ""
    for k, v in OFFICIAL_MEDIA_NAMES.items():
        if k in media_hint or media_hint in k:
            media_final = v
            break
    
    if not media_final:
        for ch, kws in MEGA_CHANNELS.items():
            if ch in media_hint:
                media_final = OFFICIAL_MEDIA_NAMES.get(ch, ch)
                break
    
    if not media_final:
        print(f"    âŒ Media Not Found")
        return None
    
    city = place_info['address'].split()[1] if len(place_info['address'].split()) > 1 else ""
    menu = place_info['category']
    
    # 21ëŒ€ ì§€ì¹¨: 3-Step ì´ë¯¸ì§€ í™•ë³´
    image_url = get_image_3step(place_info['name'], menu, city)
    
    if not image_url:
        print(f"    âŒ Zero Tolerance: No Image")
        return None
    
    place_info['media'] = media_final
    # 21ëŒ€ ì§€ì¹¨: ë¯¸ë””ì–´ëª… ì¤‘ë³µ ê¸ˆì§€, ë©”ë‰´ ì„¤ëª…ë§Œ
    place_info['description'] = f"{menu} ì „ë¬¸ì ."
    place_info['image_url'] = image_url
    place_info['naver_url'] = f"https://map.naver.com/p/search/{urllib.parse.quote(place_info['name'])}"
    
    addr_parts = place_info['address'].split(' ')
    place_info['addressProvince'] = addr_parts[0] if len(addr_parts) > 0 else ""
    place_info['addressCity'] = addr_parts[1] if len(addr_parts) > 1 else ""
    place_info['addressDistrict'] = addr_parts[2] if len(addr_parts) > 2 else ""
    
    print(f"    âœ… SUCCESS: {place_info['name']} ({place_info['category']})")
    return place_info

def collect_main(target_count=20):
    """21ëŒ€ ì§€ì¹¨ ìˆ˜ì§‘ ë©”ì¸"""
    print("ğŸš€ [Phase 49] 21ëŒ€ ìˆ˜ì§‘ ì§€ì¹¨ ì—”ì§„ ì‹œì‘")
    print(f"   Target: {target_count}ê°œ ìˆ˜ì§‘\n")
    
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    src_path = os.path.join(root_dir, 'src', 'data', 'places.json')
    
    try:
        with open(src_path, 'r', encoding='utf-8') as f:
            existing_places = json.load(f)
    except:
        existing_places = []
    
    seen_keys = set()
    for p in existing_places:
        seen_keys.add(f"{p['name']}_{p['address']}")
    
    new_places = []
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
    search_keywords = []
    for ytb, kws in MEGA_CHANNELS.items():
        main_kw = kws[1] if len(kws) > 1 else kws[0]
        search_keywords.append(f"{main_kw} ë§›ì§‘ ì¶”ì²œ")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        for seed in search_keywords:
            if len(new_places) >= target_count:
                break
            
            print(f"\nğŸ“¡ Searching: {seed}")
            candidates = search_blog_first(seed)
            if not candidates:
                continue
            
            media_hint = seed.split()[0]
            
            futures = []
            for cand in candidates[:10]:  # ìƒìœ„ 10ê°œë§Œ
                futures.append(executor.submit(process_candidate, cand, media_hint, seen_keys))
            
            for f in concurrent.futures.as_completed(futures):
                if len(new_places) >= target_count:
                    break
                
                res = f.result()
                if res:
                    key = f"{res['name']}_{res['address']}"
                    if key not in seen_keys:
                        res['id'] = len(existing_places) + len(new_places) + 1
                        new_places.append(res)
                        seen_keys.add(key)
                        print(f"\n  âœ¨ [{len(new_places)}/{target_count}] {res['name']} - {res['media']}")
    
    if new_places:
        with open(src_path, 'w', encoding='utf-8') as f:
            json.dump(existing_places + new_places, f, ensure_ascii=False, indent=2)
        print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(new_places)}ê°œ ì¶”ê°€")
        print(f"   Total: {len(existing_places) + len(new_places)}ê°œ")
    else:
        print("\n\nâš ï¸ ì‹ ê·œ ìˆ˜ì§‘ ì—†ìŒ")

if __name__ == "__main__":
    collect_main(target_count=20)
