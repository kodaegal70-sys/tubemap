#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¹„ìš© ìµœì í™” YouTube ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- ì •ê·œì‹ íŒŒì‹± ìš°ì„  (70% ì„±ê³µë¥ , ë¬´ë£Œ)
- OpenAIëŠ” fallback (30%, ìµœì†Œ ë¹„ìš©)
- ìƒìœ„ 20ê°œ ì±„ë„ë§Œ ì§‘ì¤‘
- ìµœê·¼ 30ê°œ ì˜ìƒë§Œ
"""

import os
import json
import re
import requests
from typing import Optional, Dict, List

# .env.local ì§ì ‘ íŒŒì‹±
def load_env():
    env_path = os.path.join(os.path.dirname(__file__), '..', '.env.local')
    env_vars = {}
    try:
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key] = value
    except:
        pass
    return env_vars

env = load_env()

# API Keys
YOUTUBE_API_KEY = env.get('YOUTUBE_API_KEY')
KAKAO_API_KEY = env.get('KAKAO_REST_API_KEY')
OPENAI_API_KEY = env.get('OPENAI_API_KEY')

# ìƒìœ„ 20ê°œ ì±„ë„ë§Œ (êµ¬ë…ì ê¸°ì¤€)
TOP_CHANNELS = [
    "tzuyang", "ë¬¸ë³µí¬ Eat with Boki", "í–„ì§€ Hamzy", "ì˜ì˜ Ssoyoung",
    "ë°´ì¯” Banzz", "íˆë°¥ Heebab", "ë°±ì¢…ì›ì˜ ìš”ë¦¬ë¹„ì±…", "ë§›ìˆëŠ” ë…€ì„ë“¤ OFFICIAL",
    "ì„±ì‹œê²½ SUNG SI KYUNG", "ì˜êµ­ë‚¨ì Korean Englishman", "ê³½íŠœë¸Œ", "ìµœìë¡œë“œ",
    "ë˜ê°„ì§‘", "ìƒí•´ê¸° SangHaegi", "ì•¼ë¯¸ë³´ì´ Yummyboy", "ì…ì§§ì€í–‡ë‹˜",
    "ChefPaikTV", "ìŠ¤íŠ¸ë¦¬íŠ¸í‘¸ë“œíŒŒì´í„° tvN", "ì§‘ë°¥ë°±ì„ ìƒ OFFICIAL", "ìš”ë¦¬ë³´ê³  ì¡°ë¦¬ë³´ê³ "
]

def extract_with_regex(text: str) -> List[Dict]:
    """ì •ê·œì‹ìœ¼ë¡œ ì„¤ëª…ë€ íŒŒì‹± (ë¬´ë£Œ, ë¹ ë¦„)"""
    candidates = []
    
    # íŒ¨í„´ 1: ğŸ“ ìœ„ì¹˜/ì£¼ì†Œ
    location_patterns = [
        r'ğŸ“\s*(.+?)(?=\n|ğŸ“|ğŸ´|$)',
        r'ìœ„ì¹˜[:\s]+(.+?)(?=\n|ì „í™”|ë©”ë‰´|$)',
        r'ì£¼ì†Œ[:\s]+(.+?)(?=\n|ì „í™”|ë©”ë‰´|$)',
        r'ì¥ì†Œ[:\s]+(.+?)(?=\n|ì „í™”|ë©”ë‰´|$)'
    ]
    
    # íŒ¨í„´ 2: ğŸ“ ì „í™”ë²ˆí˜¸
    phone_pattern = r'(\d{2,3}[-\s]?\d{3,4}[-\s]?\d{4})'
    
    # íŒ¨í„´ 3: ğŸ´ ë©”ë‰´
    menu_patterns = [
        r'ğŸ´\s*(.+?)(?=\n|ğŸ“|ğŸ“|$)',
        r'ë©”ë‰´[:\s]+(.+?)(?=\n|ì „í™”|ìœ„ì¹˜|$)',
        r'ëŒ€í‘œë©”ë‰´[:\s]+(.+?)(?=\n|$)'
    ]
    
    # íŒ¨í„´ 4: ìƒí˜¸ëª… (ëŒ€ê´„í˜¸, ë”°ì˜´í‘œ)
    name_patterns = [
        r'\[([ê°€-í£a-zA-Z0-9\s]{2,20})\]',
        r'\"([ê°€-í£a-zA-Z0-9\s]{2,20})\"',
        r'ê°€ê²Œëª…[:\s]+([ê°€-í£a-zA-Z0-9\s]{2,20})',
        r'ì—…ì²´ëª…[:\s]+([ê°€-í£a-zA-Z0-9\s]{2,20})'
    ]
    
    # ì¶”ì¶œ
    store_name = None
    address = None
    menu = None
    
    for pattern in name_patterns:
        match = re.search(pattern, text)
        if match:
            store_name = match.group(1).strip()
            break
    
    for pattern in location_patterns:
        match = re.search(pattern, text)
        if match:
            address = match.group(1).strip()
            break
    
    for pattern in menu_patterns:
        match = re.search(pattern, text)
        if match:
            menu = match.group(1).strip()
            break
    
    # ìµœì†Œ ì¡°ê±´: ìƒí˜¸ëª… ë˜ëŠ” ì£¼ì†Œ ì¤‘ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
    if store_name or address:
        # ì§€ì—­ íŒíŠ¸ ì¶”ì¶œ
        area_hint = ''
        if address:
            area_parts = address.split()
            if len(area_parts) >= 2:
                area_hint = f"{area_parts[0]} {area_parts[1]}"
        
        candidates.append({
            'store_name_raw': store_name or '',
            'menu_hint': [menu] if menu else [],
            'area_hint': area_hint,
            'address_hint': address or '',
            'extraction_method': 'regex'
        })
    
    return candidates

def extract_with_openai(text: str) -> List[Dict]:
    """OpenAIë¡œ ì¶”ì¶œ (fallback, ë¹„ìš© ë°œìƒ)"""
    try:
        headers = {
            'Authorization': f'Bearer {OPENAI_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        prompt = f"""ë‹¤ìŒ YouTube ì˜ìƒ ì •ë³´ì—ì„œ ë§›ì§‘ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

{text[:500]}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
[{{"store_name": "ì—…ì²´ëª…", "menu": "ë©”ë‰´", "area": "ì§€ì—­", "address": "ì£¼ì†Œ"}}]
ë§›ì§‘ ì—†ìœ¼ë©´ []"""
        
        payload = {
            'model': 'gpt-3.5-turbo',
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'temperature': 0.3,
            'max_tokens': 300
        }
        
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers=headers,
            json=payload,
            timeout=15
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            # JSON íŒŒì‹±
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
            
            places = json.loads(content)
            
            candidates = []
            for p in places:
                candidates.append({
                    'store_name_raw': p.get('store_name', ''),
                    'menu_hint': [p.get('menu', '')] if p.get('menu') else [],
                    'area_hint': p.get('area', ''),
                    'address_hint': p.get('address', ''),
                    'extraction_method': 'openai'
                })
            
            return candidates
        
        return []
    
    except Exception as e:
        print(f"  âš ï¸ OpenAI ì˜¤ë¥˜: {e}")
        return []

def step5_attach_google_photo(place: Dict) -> Dict:
    """Step 5: êµ¬ê¸€ Places ì‚¬ì§„ ë¶™ì´ê¸°"""
    print(f"\n[Step 5] êµ¬ê¸€ ì‚¬ì§„: {place['store_name']}")
    
    # Google Places APIë¡œ ì¥ì†Œ ê²€ìƒ‰
    url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
    params = {
        'input': f"{place['store_name']} {place['address']}",
        'inputtype': 'textquery',
        'fields': 'place_id,photos',
        'key': env.get('GOOGLE_PLACES_API_KEY')
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if data.get('status') == 'OK' and data.get('candidates'):
            candidate = data['candidates'][0]
            google_place_id = candidate.get('place_id')
            
            # ì‚¬ì§„ì´ ìˆìœ¼ë©´ URL ìƒì„±
            if candidate.get('photos'):
                photo_reference = candidate['photos'][0]['photo_reference']
                photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_reference}&key={env.get('GOOGLE_PLACES_API_KEY')}"
                
                print(f"  âœ… ì´ë¯¸ì§€ í™•ë³´: Google Places")
                return {
                    'google_place_id': google_place_id,
                    'hero_food_image': photo_url,
                    'hero_food_image_source': 'google_places_photo'
                }
            else:
                print(f"  âš ï¸ ì´ë¯¸ì§€ ì—†ìŒ (Google Placesì— ì‚¬ì§„ ì—†ìŒ)")
                return {
                    'google_place_id': google_place_id,
                    'hero_food_image': None,
                    'hero_food_image_source': 'null'
                }
        else:
            print(f"  âš ï¸ Google Places ë§¤ì¹­ ì‹¤íŒ¨")
            return {
                'google_place_id': None,
                'hero_food_image': None,
                'hero_food_image_source': 'null'
            }
    
    except Exception as e:
        print(f"  âŒ Google Places API ì˜¤ë¥˜: {e}")
        return {
            'google_place_id': None,
            'hero_food_image': None,
            'hero_food_image_source': 'null'
        }

def step1_identify_channel(channel_name: str) -> Optional[Dict]:
    """Step 1: ì±„ë„ ì‹ë³„"""
    print(f"\n[Step 1] ì±„ë„: {channel_name}")
    
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        'part': 'snippet',
        'q': channel_name,
        'type': 'channel',
        'maxResults': 1,
        'key': YOUTUBE_API_KEY
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'items' in data and len(data['items']) > 0:
            channel = data['items'][0]
            return {
                'channel_name': channel['snippet']['title'],
                'channel_id': channel['snippet']['channelId']
            }
    except:
        pass
    
    return None

def step2_collect_videos(channel_id: str, max_results=30) -> List[Dict]:
    """Step 2: ìµœì‹  30ê°œ ì˜ìƒë§Œ ìˆ˜ì§‘ (í•„í„°ë§)"""
    print(f"[Step 2] ì˜ìƒ ìˆ˜ì§‘ (ìµœëŒ€ {max_results}ê°œ)")
    
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        'part': 'snippet',
        'channelId': channel_id,
        'order': 'date',
        'type': 'video',
        'maxResults': max_results,
        'key': YOUTUBE_API_KEY
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'items' not in data:
            return []
        
        videos = []
        keywords = ['ë§›ì§‘', 'ë¨¹ë°©', 'ì‹ë‹¹', 'ìŒì‹', 'ì¹´í˜', 'ë§›', 'ë¨¹']
        
        for item in data['items']:
            title = item['snippet']['title']
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            if any(kw in title for kw in keywords):
                videos.append({
                    'video_id': item['id']['videoId'],
                    'title': title,
                    'description': item['snippet']['description'],
                    'video_url': f"https://www.youtube.com/watch?v={item['id']['videoId']}"
                })
        
        print(f"  âœ… {len(videos)}ê°œ ìˆ˜ì§‘ (í•„í„°ë§ í›„)")
        return videos
    
    except:
        return []

def step3_extract_smart(video: Dict) -> List[Dict]:
    """Step 3: ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ (ì •ê·œì‹ ìš°ì„  â†’ OpenAI fallback)"""
    print(f"\n[Step 3] {video['title'][:40]}...")
    
    text = f"{video['title']}\n\n{video['description']}"
    
    # 1ì°¨: ì •ê·œì‹ (ë¬´ë£Œ)
    candidates = extract_with_regex(text)
    
    if candidates:
        print(f"  âœ… {len(candidates)}ê°œ ì¶”ì¶œ (ì •ê·œì‹, ë¬´ë£Œ)")
        for c in candidates:
            c['source_video_id'] = video['video_id']
        return candidates
    
    # 2ì°¨: OpenAI (ë¹„ìš© ë°œìƒ)
    print(f"  âš™ï¸ OpenAI í˜¸ì¶œ ì¤‘...")
    candidates = extract_with_openai(text)
    
    if candidates:
        print(f"  âœ… {len(candidates)}ê°œ ì¶”ì¶œ (OpenAI, $0.002)")
        for c in candidates:
            c['source_video_id'] = video['video_id']
        return candidates
    
    print(f"  âŒ ì¶”ì¶œ ì‹¤íŒ¨")
    return []

def step4_confirm_kakao(candidate: Dict) -> Optional[Dict]:
    """Step 4: ì¹´ì¹´ì˜¤ í™•ì • + í”„ëœì°¨ì´ì¦ˆ í•„í„°"""
    query = candidate['store_name_raw'] or candidate['address_hint']
    if not query:
        return None
    
    if candidate.get('area_hint'):
        query = f"{query} {candidate['area_hint']}"
    
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": query, "size": 1}
    
    # ê°•í™”ëœ í”„ëœì°¨ì´ì¦ˆ ë° ì¼ë°˜ëª…ì‚¬ í‚¤ì›Œë“œ (v4.1)
    EXCLUDE_KEYWORDS = [
        'KFC', 'ë§¥ë„ë‚ ë“œ', 'ë²„ê±°í‚¹', 'ë¡¯ë°ë¦¬ì•„', 'ë§˜ìŠ¤í„°ì¹˜',
        'êµì´Œ', 'BBQ', 'bhc', 'êµ½ë„¤', 'í‘¸ë¼ë‹­', '60ê³„',
        'ìŠ¤íƒ€ë²…ìŠ¤', 'íˆ¬ì¸', 'ì´ë””ì•¼', 'ë©”ê°€ì»¤í”¼', 'ì»´í¬ì¦ˆ', 'ë¹½ë‹¤ë°©', 'ìš°ì§€ì»¤í”¼',
        'CU', 'GS25', 'ì„¸ë¸ì¼ë ˆë¸', 'í¸ì˜ì ',
        'ì‹ ì „', 'ì—½ê¸°ë–¡ë³¶ì´', 'ì‘ê¸‰ì‹¤ë–¡ë³¶ì´',
        'ë§›ì§‘', 'ë¨¹ë°©', 'ì›ì¡°', 'ì™•ëˆê¹ŒìŠ¤', 'ë¶ˆë‹­ë°œ' # ì¼ë°˜ ëª…ì‚¬ì„± ìƒí˜¸ ì œì™¸
    ]
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        data = response.json()
        
        if data.get('documents'):
            place = data['documents'][0]
            category = place.get('category_name', '')
            store_name = place['place_name']
            
            # í”„ëœì°¨ì´ì¦ˆ ë° ì¼ë°˜ëª…ì‚¬ í•„í„°
            if any(kw in store_name for kw in EXCLUDE_KEYWORDS):
                print(f"  âŒ í•„í„° ì œì™¸: {store_name}")
                return None
            
            # ìŒì‹ì /ì¹´í˜ë§Œ í—ˆìš©
            if 'ìŒì‹ì ' in category or 'ì¹´í˜' in category:
                # v4.0: ì—­ë°©í–¥ ì£¼ì†Œ ê²€ì¦ (ì¼ë°˜ ëª…ì‚¬ ì œì™¸)
                address = place['address_name']
                reverse_params = {"query": address, "size": 5}
                
                try:
                    reverse_response = requests.get(url, headers=headers, params=reverse_params, timeout=10)
                    reverse_data = reverse_response.json()
                    
                    # ì£¼ì†Œë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ì— ìƒí˜¸ëª…ì´ ìˆëŠ”ì§€ í™•ì¸
                    if reverse_data.get('documents'):
                        reverse_names = [d['place_name'] for d in reverse_data['documents']]
                        
                        # ìƒí˜¸ëª…ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìœ¼ë©´ ì¼ë°˜ ëª…ì‚¬ë¡œ íŒë‹¨
                        if store_name not in reverse_names:
                            print(f"  âŒ ì—­ë°©í–¥ ê²€ì¦ ì‹¤íŒ¨: {store_name} (ì¼ë°˜ ëª…ì‚¬)")
                            return None
                except:
                    pass  # ì—­ë°©í–¥ ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì§„í–‰
                
                return {
                    'kakao_place_id': place['id'],
                    'store_name': store_name,
                    'category': category.split('>')[-1].strip(),
                    'address': address,
                    'lat': float(place['y']),
                    'lng': float(place['x']),
                    'phone': place.get('phone', None),
                    'reverse_verified': True
                }
    except:
        pass
    
    return None

def step6_get_youtube_review(video_id: str, store_name: str, category: str) -> str:
    """Step 6: YouTube ëŒ“ê¸€ ë¦¬ë·° (v4.0)"""
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        'part': 'snippet',
        'videoId': video_id,
        'order': 'relevance',
        'maxResults': 20,
        'key': YOUTUBE_API_KEY
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'items' in data:
            store_keywords = store_name.replace('ì ', '').split()
            menu_keywords = category.replace(',', ' ').split()
            food_keywords = ['ë§›ìˆ', 'ë§›', 'ìŒì‹', 'ë¨¹', 'ë©”ë‰´', 'ì¶”ì²œ', 'ìµœê³ ', 'ëŒ€ë°•']
            
            for item in data['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comment = re.sub(r'<[^>]+>', '', comment)
                
                if not re.search(r'[ê°€-í£]', comment):
                    continue
                
                has_store = any(kw in comment for kw in store_keywords if len(kw) > 1)
                has_menu = any(kw in comment for kw in menu_keywords if len(kw) > 1)
                has_food = any(kw in comment for kw in food_keywords)
                
                if (has_store or has_menu or has_food) and len(comment) <= 60:
                    comment = re.sub(r'[^\w\sê°€-í£.,!?]', '', comment).strip()
                    if comment and len(comment) >= 5:
                        return comment
    except:
        pass
    
    # Fallback: ëŒ€í‘œ ë©”ë‰´
    return category.split(',')[0].strip()

def process_channel_optimized(channel_name: str):
    """ìµœì í™”ëœ ì±„ë„ ì²˜ë¦¬"""
    print(f"\n{'='*60}")
    print(f"ì±„ë„: {channel_name}")
    print(f"{'='*60}")
    
    channel_info = step1_identify_channel(channel_name)
    if not channel_info:
        return []
    
    videos = step2_collect_videos(channel_info['channel_id'], max_results=30)
    if not videos:
        return []
    
    results = []
    regex_count = 0
    openai_count = 0
    
    for video in videos[:10]:  # í…ŒìŠ¤íŠ¸: 10ê°œë§Œ
        candidates = step3_extract_smart(video)
        
        for candidate in candidates[:1]:  # í›„ë³´ 1ê°œë§Œ
            if candidate.get('extraction_method') == 'regex':
                regex_count += 1
            else:
                openai_count += 1
            
            place = step4_confirm_kakao(candidate)
            if place:
                # Step 6: YouTube ëŒ“ê¸€ ë¦¬ë·°
                review = step6_get_youtube_review(
                    video['video_id'],
                    place['store_name'],
                    place['category']
                )
                
                record = {
                    'source': {
                        'channel_name': channel_info['channel_name'],
                        'video_id': video['video_id'],
                        'video_url': video['video_url']
                    },
                    'place': place,
                    'review': review,
                    'extraction_method': candidate.get('extraction_method', 'unknown')
                }
                results.append(record)
                print(f"  âœ… {place['store_name']} - {review[:30]}...")
    
    print(f"\nğŸ“Š í†µê³„: ì •ê·œì‹={regex_count}, OpenAI={openai_count}")
    print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${openai_count * 0.002:.3f}")
    
    return results

if __name__ == "__main__":
    print("ğŸš€ ë¹„ìš© ìµœì í™” YouTube ìˆ˜ì§‘ ì‹œìŠ¤í…œ (ìƒ˜í”Œ 5ê°œ)")
    print(f"   ëŒ€ìƒ: ìƒìœ„ {len(TOP_CHANNELS)}ê°œ ì±„ë„")
    print(f"   ëª©í‘œ: 5ê°œ ë§›ì§‘ ìˆ˜ì§‘ (í”„ëœì°¨ì´ì¦ˆ ì œì™¸)")
    
    all_results = []
    target_count = 5
    
    # 5ê°œ ìˆ˜ì§‘ë  ë•Œê¹Œì§€ ì±„ë„ ìˆœíšŒ
    for channel_name in TOP_CHANNELS:
        if len(all_results) >= target_count:
            break
        
        print(f"\ní˜„ì¬ ìˆ˜ì§‘: {len(all_results)}/{target_count}")
        results = process_channel_optimized(channel_name)
        all_results.extend(results)
    
    # ì‚¬ì§„ ìˆ˜ì§‘ ë° ìµœì¢… ë³€í™˜
    final_places = []
    for idx, item in enumerate(all_results):
        # Step 5: Google Places Photo
        photo_info = step5_attach_google_photo(item['place'])
        
        # places.json í¬ë§·ìœ¼ë¡œ ë³€í™˜
        addr_parts = item['place']['address'].split()
        
        place_record = {
            "id": idx + 1,
            "name": item['place']['store_name'],
            "lat": item['place']['lat'],
            "lng": item['place']['lng'],
            "media": item['source']['channel_name'],
            "description": item['review'],
            "address": item['place']['address'],
            "phone": item['place'].get('phone', ''),
            "image_url": photo_info.get('hero_food_image'),
            "naver_url": f"https://map.naver.com/p/search/{item['place']['store_name']}",
            "category": item['place']['category'],
            "addressProvince": addr_parts[0] if len(addr_parts) > 0 else "",
            "addressCity": addr_parts[1] if len(addr_parts) > 1 else "",
            "addressDistrict": addr_parts[2] if len(addr_parts) > 2 else "",
            "category_group": "",
            "road_address": "",
            "source_video_url": item['source']['video_url'],
            "google_place_id": photo_info.get('google_place_id')
        }
        final_places.append(place_record)
    
    # 1. youtube_sample_5.json ì €ì¥ (ì›ë³¸ ë¡œê·¸)
    output_path = os.path.join(os.path.dirname(__file__), '..', 'youtube_sample_5.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # 2. src/data/places.json ì €ì¥ (UIìš©)
    ui_data_path = os.path.join(os.path.dirname(__file__), '..', 'src', 'data', 'places.json')
    with open(ui_data_path, 'w', encoding='utf-8') as f:
        json.dump(final_places, f, ensure_ascii=False, indent=2)
    
    print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(final_places)}ê°œ")
    print(f"   ì›ë³¸ ì €ì¥: {output_path}")
    print(f"   UI ë°ì´í„° ì €ì¥: {ui_data_path}")
    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: node scripts/sync_to_supabase.js ì‹¤í–‰")
