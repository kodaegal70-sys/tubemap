import requests
import json
import time
import os
import urllib.request
import urllib.parse

# API ì„¤ì •
KAKAO_API_KEY = "c6088c2c7ec5f0e1ed1122ba613db0fb"
NAVER_CLIENT_ID = "Ug7csvlUDeTe1I72ehKQ"
NAVER_CLIENT_SECRET = "pqPjVw9kig"

EXCLUDE_CATEGORIES = ["ìŠˆí¼ë§ˆì¼“", "ëŒ€í˜•ë§ˆíŠ¸", "ì§€í•˜ì² ì—­", "ê¸°ì°¨ì—­", "ì˜¨ì²œ", "ëª©ìš•íƒ•", "ì‚¬ìš°ë‚˜", "ìœ ì‹íŒë§¤", "ì¬ê±´ì¶•", "ëª¨ë¸í•˜ìš°ìŠ¤", "ë°±í™”ì ", "ë©´ì„¸ì ", "í¸ì˜ì "]
FRANCHISE_KEYWORDS = ["ìŠ¤íƒ€ë²…ìŠ¤", "ë§¥ë„ë‚ ë“œ", "ë¦¬ì•„", "í‚¹", "ì¨ë¸Œì›¨ì´", "íˆ¬ì¸", "íŒŒìŠ¤ì¿ ì°Œ", "ì´ë””ì•¼", "ë©”ê°€ì»¤í”¼", "ì»´í¬ì¦ˆ", "ë¹½ë‹¤ë°©", "ë°°ìŠ¤í‚¨", "ë˜í‚¨", "íŒŒë¦¬ë°”ê²Œëœ¨", "ëšœë ˆì¥¬ë¥´", "ì„œê°€ì•¤ì¿¡", "ì•„ì›ƒë°±", "ë¹•ìŠ¤", "ë³¸ì£½", "í•œì†¥", "ë´‰êµ¬ìŠ¤"]

def get_kakao_search(query: str, region: str = "í‰íƒ"):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    results = []
    
    # í‰íƒ ì£¼ìš” ê±°ì 
    search_points = ["í‰íƒì‹œì²­", "í‰íƒì—­", "ì§€ì œì—­", "ì„œì •ë¦¬ì—­", "ì†¡íƒ„ì—­", "íŒ½ì„±ì", "ì•ˆì¤‘ì", "í¬ìŠ¹ì", "ê³ ë•ë™"]
    
    for point in search_points:
        for page in range(1, 4):
            params = {"query": f"{point} {query}", "page": page, "size": 15}
            try:
                res = requests.get(url, headers=headers, params=params)
                data = res.json()
                if not data['documents']: break
                for doc in data['documents']:
                    if "í‰íƒ" not in doc['address_name']: continue
                    results.append({
                        "name": doc['place_name'],
                        "lat": float(doc['y']), "lng": float(doc['x']),
                        "address": doc['address_name'], "phone": doc.get('phone', ''),
                        "category": doc.get('category_name', '').split('>')[-1].strip(),
                        "road_address": doc.get('road_address_name', '')
                    })
                time.sleep(0.1)
            except: break
    return results

def verify_media(name: str):
    query_prefix = "í‰íƒ " + name
    query_suffix = " ë°©ì†¡ ì¶œì—° ë§›ì§‘"
    encText = urllib.parse.quote(query_prefix + query_suffix)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display=15"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    media = []
    try:
        res = urllib.request.urlopen(req)
        data = json.loads(res.read().decode('utf-8'))
        full_text = " ".join([i['title'] + i['description'] for i in data['items']]).replace('<b>', '').replace('</b>', '')
        
        # ë§›ì§‘ í‚¤ì›Œë“œ
        keywords = {
            "ë°±ë°˜ê¸°í–‰": "ì‹ê° í—ˆì˜ë§Œì˜ ë°±ë°˜ê¸°í–‰", "ìƒí™œì˜ ë‹¬ì¸": "ìƒí™œì˜ ë‹¬ì¸", "ë§›ìˆëŠ” ë…€ì„ë“¤": "ë§›ìˆëŠ” ë…€ì„ë“¤",
            "ìƒìƒì •ë³´": "ìƒìƒì •ë³´", "ì¯”ì–‘": "ì¯”ì–‘ (ìœ íŠœë¸Œ)", "í’ì": "í’ì ë˜ê°„ì§‘", "ë˜ê°„ì§‘": "í’ì ë˜ê°„ì§‘",
            "ì„±ì‹œê²½": "ì„±ì‹œê²½ ë¨¹ì„í…ë°", "ë°±ì¢…ì›": "ë°±ì¢…ì› 3ëŒ€ì²œì™•"
        }
        for k, v in keywords.items():
            if k in full_text: media.append(v)
                
    except: pass
    return "|".join(list(set(media)))

def collect():
    print("ğŸš€ [í‰íƒ í†µí•© ìˆ˜ì§‘] ì‹œì‘ (ë§›ì§‘ ì „ë¬¸ ëª¨ë“œ)")
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    src_path = os.path.join(root_dir, 'src', 'data', 'places.json')
    
    with open(src_path, 'r', encoding='utf-8') as f:
        existing_places = json.load(f)
    
    seen_keys = set(f"{p['name']}_{p['address']}" for p in existing_places)
    last_id = max(p['id'] for p in existing_places) if existing_places else 0
    
    new_places = []
    
    # 1. ë§›ì§‘ 50ì„  ìˆ˜ì§‘ (í”„ëœì°¨ì´ì¦ˆ ì œì™¸ 50ê°œ ë‹¬ì„± ì‹œë„)
    print("ğŸ± ë§›ì§‘ ìˆ˜ì§‘ ì¤‘ (í”„ëœì°¨ì´ì¦ˆ ì°¨ë‹¨ ëª¨ë“œ)...")
    rest_keywords = ["ë§›ì§‘", "ë…¸í¬", "ë°©ì†¡ì¶œì—°", "ì‹ë‹¹", "ì¹´í˜"]
    rest_candidates = []
    for k in rest_keywords:
        rest_candidates.extend(get_kakao_search(k, "í‰íƒ"))
    
    current_pt_rests = [p for p in existing_places if p.get('addressCity') == 'í‰íƒì‹œ' and p.get('category') != 'ì´¬ì˜ì§€']
    total_needed = 50
    rest_count = len(current_pt_rests)
    print(f"  ğŸ’¡ í˜„ì¬ í‰íƒ ì‹ë‹¹: {rest_count}ê°œ. ë¶€ì¡±ë¶„({total_needed - rest_count}ê°œ) ì¶”ê°€ ìˆ˜ì§‘ ì‹œì‘.")

    for c in rest_candidates:
        if rest_count >= 55: break
        key = f"{c['name']}_{c['address']}"
        if key in seen_keys: continue
        if c['category'] in EXCLUDE_CATEGORIES: continue
        if any(fk in c['name'] for fk in FRANCHISE_KEYWORDS): continue
        
        media = verify_media(c['name'], is_scenery=False)
        if media:
            rest_count += 1
            last_id += 1
            new_places.append({
                "id": last_id,
                "name": c['name'], "lat": c['lat'], "lng": c['lng'],
                "media": media,
                "description": f"{media}ì— ì†Œê°œëœ í‰íƒ ë§›ì§‘",
                "address": c['address'], "phone": c['phone'], "image_url": "",
                "naver_url": f"https://map.naver.com/p/search/{urllib.parse.quote(c['name'])}",
                "category": c['category'], "addressProvince": "ê²½ê¸°", "addressCity": "í‰íƒì‹œ",
                "addressDistrict": c['road_address'].split(' ')[2] if len(c['road_address'].split(' ')) > 2 else ""
            })
            seen_keys.add(key)
            print(f"  ğŸ± [{rest_count}/50] {c['name']} ({media})")
            time.sleep(0.1)
            
    # í†µí•© ì €ì¥
    with open(src_path, 'w', encoding='utf-8') as f:
        json.dump(existing_places + new_places, f, ensure_ascii=False, indent=2)
        
    print(f"\nğŸ‰ í‰íƒ ë°ì´í„° í†µí•© ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(new_places)}ê°œ ì¶”ê°€")

if __name__ == "__main__":
    collect()
