import requests
import json
import time
import os
import urllib.request
import urllib.parse
import re

# API ì„¤ì •
KAKAO_API_KEY = "c6088c2c7ec5f0e1ed1122ba613db0fb"
NAVER_CLIENT_ID = "Ug7csvlUDeTe1I72ehKQ"
NAVER_CLIENT_SECRET = "pqPjVw9kig"

EXCLUDE_CATEGORIES = ["ìŠˆí¼ë§ˆì¼“", "ëŒ€í˜•ë§ˆíŠ¸", "ì§€í•˜ì² ì—­", "ê¸°ì°¨ì—­", "ì˜¨ì²œ", "ëª©ìš•íƒ•", "ì‚¬ìš°ë‚˜", "ìœ ì‹íŒë§¤", "ì¬ê±´ì¶•", "ëª¨ë¸í•˜ìš°ìŠ¤", "ë°±í™”ì ", "ë©´ì„¸ì ", "í¸ì˜ì ", "í…Œë§ˆê±°ë¦¬", "ê´€ê´‘ì•ˆë‚´ì†Œ", "ê³µì›", "ì‹œì¥", "ê±°ë¦¬"]
FRANCHISE_KEYWORDS = ["ìŠ¤íƒ€ë²…ìŠ¤", "ë§¥ë„ë‚ ë“œ", "ë¦¬ì•„", "í‚¹", "ì¨ë¸Œì›¨ì´", "íˆ¬ì¸", "íŒŒìŠ¤ì¿ ì°Œ", "ì´ë””ì•¼", "ë©”ê°€ì»¤í”¼", "ì»´í¬ì¦ˆ", "ë¹½ë‹¤ë°©", "ë°°ìŠ¤í‚¨", "ë˜í‚¨", "íŒŒë¦¬ë°”ê²Œëœ¨", "ëšœë ˆì¥¬ë¥´", "ì„œê°€ì•¤ì¿¡", "ì•„ì›ƒë°±", "ë¹•ìŠ¤", "ë³¸ì£½", "í•œì†¥", "ë´‰êµ¬ìŠ¤"]

def get_kakao_search(query: str):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": query, "size": 15}
    try:
        res = requests.get(url, headers=headers, params=params)
        data = res.json()
        if not data.get('documents'): return []
        results = []
        for doc in data['documents']:
            results.append({
                "name": doc['place_name'],
                "lat": float(doc['y']), "lng": float(doc['x']),
                "address": doc['address_name'], "phone": doc.get('phone', ''),
                "category": doc.get('category_name', '').split('>')[-1].strip(),
                "road_address": doc.get('road_address_name', '')
            })
        return results
    except: return []

def verify_media_latest(name: str):
    # ìµœì‹ ì„±(2024)ì„ ê°•ì¡°í•˜ì—¬ ê²€ìƒ‰
    query = f"2024 2025 ë§›ì§‘ {name} ë°©ì†¡ ì¶œì—° ìœ íŠœë¸Œ"
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display=10&sort=sim"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    media = []
    try:
        res = urllib.request.urlopen(req)
        data = json.loads(res.read().decode('utf-8'))
        full_text = " ".join([i['title'] + i['description'] for i in data['items']]).replace('<b>', '').replace('</b>', '')
        
        keywords = {
            "ë°±ë°˜ê¸°í–‰": "ì‹ê° í—ˆì˜ë§Œì˜ ë°±ë°˜ê¸°í–‰", "ìƒí™œì˜ ë‹¬ì¸": "ìƒí™œì˜ ë‹¬ì¸", "ë§›ìˆëŠ” ë…€ì„ë“¤": "ë§›ìˆëŠ” ë…€ì„ë“¤",
            "ìƒìƒì •ë³´": "ìƒìƒì •ë³´", "ì¯”ì–‘": "ì¯”ì–‘ (ìœ íŠœë¸Œ)", "í’ì": "í’ì ë˜ê°„ì§‘", "ë˜ê°„ì§‘": "í’ì ë˜ê°„ì§‘",
            "ì„±ì‹œê²½": "ì„±ì‹œê²½ ë¨¹ì„í…ë°", "ë°±ì¢…ì›": "ë°±ì¢…ì› 3ëŒ€ì²œì™•", "íˆë°¥": "íˆë°¥ (ìœ íŠœë¸Œ)", "ë˜ê°„ì§‘": "í’ì ë˜ê°„ì§‘"
        }
        for k, v in keywords.items():
            if k in full_text: media.append(v)
    except: pass
    return "|".join(list(set(media)))

def collect_latest_50():
    print("ğŸš€ [ìµœì‹  ê³ í’ˆì§ˆ ë§›ì§‘ 50ì„  í™•ì¶©] ì‹œì‘")
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    src_path = os.path.join(root_dir, 'src', 'data', 'places.json')
    
    with open(src_path, 'r', encoding='utf-8') as f:
        existing_places = json.load(f)
    
    seen_keys = set(f"{p['name']}_{p['address']}" for p in existing_places)
    last_id = max(p['id'] for p in existing_places) if existing_places else 0
    
    new_places = []
    
    # ìµœì‹  ë¦¬ìŠ¤íŠ¸ ê²€ìƒ‰ìš© í‚¤ì›Œë“œ (ì§€ì—­ë³„ ë…¸í¬ ë° ì‹ ê·œ í•«í”Œë ˆì´ìŠ¤)
    # 2024ë…„ ì´í›„ ì£¼ìš” ë°©ì†¡/ìœ íŠœë¸Œ ë…¸ì¶œ ì§€ì—­
    search_queries = [
        "2024 ì„±ì‹œê²½ ë¨¹ì„í…ë° ë…¸í¬", "2024 ë°±ë°˜ê¸°í–‰ ì§€ì—­ ë§›ì§‘",
        "2025 ë§›ìˆëŠ”ë…€ì„ë“¤ ë°©ì˜", "í’ì ë˜ê°„ì§‘ 2024 ì¶”ì²œ",
        "ì œì£¼ë„ ìµœì‹  ë§›ì§‘ 2024", "ê°•ë¦‰ ìµœì‹  ë§›ì§‘ 2024", "ì „ì£¼ ìµœì‹  ë§›ì§‘ 2024"
    ]
    
    current_new_count = len([p for p in existing_places if p['id'] >= 575])
    target_total_new = 50
    need_count = target_total_new - current_new_count
    
    if need_count <= 0:
        print("âœ… ì´ë¯¸ 50ê°œì˜ ì‹ ê·œ ë°ì´í„°ê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"  ğŸ’¡ {need_count}ê°œì˜ ì¶”ê°€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    for sq in search_queries:
        if len(new_places) >= need_count: break
        print(f"ğŸ” '{sq}' ê¸°ë°˜ ìµœì‹  í›„ë³´êµ° ì¶”ì¶œ ì¤‘...")
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ì„ í†µí•´ ì—…ì²´ëª… ì¶”ì¶œ ì‹œë„
        encText = urllib.parse.quote(sq)
        url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display=30"
        
        req = urllib.request.Request(url)
        req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
        
        try:
            res = urllib.request.urlopen(req)
            data = json.loads(res.read().decode('utf-8'))
            for item in data['items']:
                if len(new_places) >= need_count: break
                text = item['title'] + " " + item['description']
                text = text.replace('<b>', '').replace('</b>', '')
                
                # ì •ê·œì‹ìœ¼ë¡œ ì—…ì²´ëª… ì¶”ì •
                potential_names = re.findall(r'\[(.*?)\]|\"(.*?)\"|\'(.*?)\'|\ã€(.*?)\ã€‘', text)
                for groups in potential_names:
                    for name in groups:
                        if name and len(name) > 1 and len(name) < 15:
                            # [ì°¨ë‹¨] ë¹„ìŒì‹ì  ë˜ëŠ” ë¬´ì˜ë¯¸í•œ ì´ë¦„
                            if any(x in name for x in ["ë˜ê°„ì§‘", "ê±°ë¦¬", "ì•ˆë‚´", "ì„¼í„°", "ì¶”ì²œ", "ë¦¬ìŠ¤íŠ¸"]): continue
                            
                            places = get_kakao_search(name)
                            for p in places:
                                if len(new_places) >= need_count: break
                                key = f"{p['name']}_{p['address']}"
                                if key in seen_keys: continue
                                if any(fk in p['name'] for fk in FRANCHISE_KEYWORDS): continue
                                
                                # ì¹´í…Œê³ ë¦¬ ë¬´ê²°ì„± ì²´í¬
                                valid_cats = ['ìŒì‹ì ', 'í•œì‹', 'ì¤‘ì‹', 'ì¼ì‹', 'ì–‘ì‹', 'ì¹´í˜', 'ë² ì´ì»¤ë¦¬', 'ìˆœëŒ€', 'êµ­ë°¥', 'íšŒ', 'ê°ˆë¹„', 'ì‚¼ê²¹ì‚´']
                                if not any(vc in p['category'] for vc in valid_cats): continue

                                media = verify_media_latest(p['name'])
                                if media:
                                    last_id += 1
                                    p['id'] = last_id
                                    p['media'] = media
                                    p['description'] = f"{media}ì— ì†Œê°œëœ ìµœì‹  í•«í”Œë ˆì´ìŠ¤ ë§›ì§‘"
                                    # ì£¼ì†Œ ë¶„í•´
                                    addr_parts = p['address'].split(' ')
                                    p['addressProvince'] = addr_parts[0] if len(addr_parts) > 0 else ""
                                    p['addressCity'] = addr_parts[1] if len(addr_parts) > 1 else ""
                                    p['addressDistrict'] = addr_parts[2] if len(addr_parts) > 2 else ""
                                    p['naver_url'] = f"https://map.naver.com/p/search/{urllib.parse.quote(p['name'])}"
                                    p['image_url'] = ""
                                    
                                    new_places.append(p)
                                    seen_keys.add(key)
                                    print(f"  âœ¨ [{len(new_places) + current_new_count}/50] {p['name']} ({media})")
                                    time.sleep(0.1)
        except: pass
        
    # ê²°ê³¼ ì €ì¥ (ì„¤ëª…ë¬¸ ì •ì œ í¬í•¨)
    if new_places:
        with open(src_path, 'w', encoding='utf-8') as f:
            json.dump(existing_places + new_places, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ìµœì‹  ë§›ì§‘ {len(new_places)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
        # ì—…ì²´ëª… ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
        print("\n--- ìˆ˜ì§‘ëœ ì—…ì²´ëª… ë¦¬ìŠ¤íŠ¸ ---")
        for p in new_places:
            print(f"- {p['name']} ({p['addressCity']})")
    else:
        print("\nâŒ ì‹ ê·œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    collect_latest_50()
